{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "def fr(test_img):\n",
    "    gray_img=cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "    harcascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    faces=harcascade.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)\n",
    "    return faces,gray_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_img=cv2.imread('C:/Users/lenovo/Desktop/MLOps  Workspace/f.jpg')\n",
    "#faces_detected, gray_img = fr(test_img)\n",
    "#print('faces_detected: ',faces_detected)\n",
    "\n",
    "#for(x,y,w,h) in faces_detected:\n",
    " #   cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=5)\n",
    "    \n",
    "#resized_img=cv2.resize(test_img,(1000,700))\n",
    "#cv2.imshow(\"face\",resized_img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_for_training_data(directory): #haarclassifier detects images and its labels the lables should be integers\n",
    "    faces=[]\n",
    "    labels=[]\n",
    "    for path,subdirname,filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.startswith(\".\"):\n",
    "                print(\"Skipping system file\")\n",
    "                continue\n",
    "            id=os.path.basename=(path)\n",
    "            img_path=os.path.join(path,filename)\n",
    "            print(\"img_path: \",img_path)\n",
    "            print('id: ',id)\n",
    "            test_img=cv2.imread(img_path)\n",
    "            if test_img is None:\n",
    "                print(\"Image Not loaded properly\")\n",
    "                continue \n",
    "            faces_rect, gray_img=fr(test_img)\n",
    "            if len(faces_rect)!=1:\n",
    "                continue  #skipping multi face images\n",
    "            (x,y,w,h)=faces_rect[0]\n",
    "            gray_crop= gray_img[y:y+w,x:x+h]\n",
    "            faces.append(gray_crop)\n",
    "            labels.append(id)\n",
    "    return faces,labels  #returning faces and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(faces,faceID):\n",
    "    face_recogn =  cv2.face_LBPHFaceRecognizer.create()\n",
    "    labels=[0]*len(faces)\n",
    "    face_recogn.train(faces, np.array(labels))\n",
    "    return face_recogn\n",
    "\n",
    "def box(test_img,face,x,y,w,h):\n",
    "    (x,y,w,h)=face\n",
    "    cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=5)\n",
    "    \n",
    "def put_text(test_img,text,x,y):\n",
    "    font             = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    bottomLeftCornerOfText = (x,y)\n",
    "    fontScale              = 1\n",
    "    fontColor              = (255,0,0)\n",
    "    lineType               = 2\n",
    "\n",
    "    cv2.putText(test_img,text, \n",
    "    bottomLeftCornerOfText, \n",
    "    font, \n",
    "    fontScale,\n",
    "    fontColor,\n",
    "    lineType)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=cv2.imread('C:/Users/lenovo/Desktop/MLOps  Workspace/Selfpractice/New folder/Face_recogn/Test_set/0/abhaa.png')\n",
    "faces_detected, gray_img = fr(test_img)\n",
    "print('faces_detected: ',faces_detected)\n",
    "faces,labels=labels_for_training_data('C:/Users/lenovo/Desktop/MLOps  Workspace/Selfpractice/New folder/Face_recogn/Test_set/')\n",
    "face_recogn=train(faces,labels)\n",
    "face_recogn.save('traindata.yml')\n",
    "name={0:'Abha',1:'Abhishek'}\n",
    "for face in faces_detected:\n",
    "    (x,y,w,h)=face\n",
    "    gray_crop=gray_img[y:y+w,x:x+h]\n",
    "    #detecting first face detected and cropping face\n",
    "    label,confidence=face_recogn.predict(gray_crop)\n",
    "    print(\"confidence: \", confidence)\n",
    "    print(\"label: \", label)\n",
    "    box(test_img,face,x,y,w,h)\n",
    "    pred_name=name[label]\n",
    "    print(pred_name)\n",
    "    put_text(test_img,pred_name,x,y)\n",
    "resized_img=cv2.resize(test_img,(1000,700))\n",
    "cv2.imshow(\"face\",resized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=cv2.imread('C:/Users/lenovo/Desktop/MLOps  Workspace/Selfpractice/New folder/Face_recogn/Test_set/0/abhaa.png')\n",
    "faces_detected, gray_img = fr(test_img)\n",
    "print('faces_detected: ',faces_detected)\n",
    "#faces,labels=labels_for_training_data('C:/Users/lenovo/Desktop/MLOps  Workspace/Selfpractice/New folder/Face_recogn/Test_set/')\n",
    "#face_recogn=train(faces,labels)\n",
    "face_recogn =  cv2.face_LBPHFaceRecognizer.create()\n",
    "face_recogn.read('traindata.yml')\n",
    "name={0:'Abha',1:'Abhishek'}\n",
    "for face in faces_detected:\n",
    "    (x,y,w,h)=face\n",
    "    gray_crop=gray_img[y:y+w,x:x+h]\n",
    "    #detecting first face detected and cropping face\n",
    "    label,confidence=face_recogn.predict(gray_crop)\n",
    "    print(\"confidence: \", confidence)\n",
    "    print(\"label: \", label)\n",
    "    box(test_img,face,x,y,w,h)\n",
    "    pred_name=name[label]\n",
    "    print(pred_name)\n",
    "    put_text(test_img,pred_name,x,y)\n",
    "resized_img=cv2.resize(test_img,(1000,700))\n",
    "cv2.imshow(\"face\",resized_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "def fr(test_img):\n",
    "    gray_img=cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "    harcascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    faces=harcascade.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)\n",
    "    return faces,gray_img\n",
    "cap=cv2.VideoCapture('http://192.168.43.1:8080/video')\n",
    "face_recogn =  cv2.face_LBPHFaceRecognizer.create()\n",
    "face_recogn.read('traindata.yml')\n",
    "name={0:'Abha',1:'Abhishek'}\n",
    "while True:\n",
    "    status, photo=cap.read()\n",
    "    test_img=photo\n",
    "    faces_detected, gray_img = fr(test_img)\n",
    "    print('faces_detected: ',faces_detected)\n",
    "   \n",
    "    for face in faces_detected:\n",
    "        (x,y,w,h)=face\n",
    "        gray_crop=gray_img[y:y+w,x:x+h]\n",
    "        #detecting first face detected and cropping face\n",
    "        label,confidence=face_recogn.predict(gray_crop)\n",
    "        print(\"confidence: \", confidence)\n",
    "        print(\"label: \", label)\n",
    "        box(test_img,face,x,y,w,h)\n",
    "        pred_name=name[label]\n",
    "        print(pred_name)\n",
    "        put_text(test_img,pred_name,x,y)\n",
    "    resized_img=cv2.resize(test_img,(1000,700))\n",
    "    cv2.imshow(\"face\",resized_img)\n",
    "    #cv2.waitKey(0)\n",
    "   # cv2.destroyAllWindows() \n",
    "  #  cv2.imshow('hi',test_img)\n",
    "    if cv2.waitKey(10)==27:\n",
    "        break \n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
